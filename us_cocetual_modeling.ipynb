{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Conceptual Modeling for User Story\n",
    "A Capstone Project for Misk Data Science Immersive Course\n",
    " \n",
    "Sarah A. AlQahtani\n",
    "\n",
    "Dec 6, 2020"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduction\n",
    "\n",
    "In software engineering, a user story is an informal, natural language description of one or more features of a software system. A user story is a tool used in Agile software development to capture a description of a software feature from an end-user perspective. A user story describes three componants:\n",
    "- Who wants the functionality\n",
    "- What functionality the end users or stakeholders want\n",
    "the system to provide\n",
    "- Why the end users and stakeholders need this functionality (optional).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset: https://data.mendeley.com/datasets/7zbk8zsd8y/1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "# load the requiered libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import random\n",
    "import pickle\n",
    "import csv\n",
    "# Reading PDFs\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "# XML\n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "# Preprocessing\n",
    "from cleantext import clean\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "from spacy.symbols import VERB, NOUN\n",
    "from spacy.matcher import Matcher\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import warnings\n",
    "#nlkt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(full_file):\n",
    "    '''\n",
    "    func: get_text(user_story.txt)\n",
    "    input:user_story.pdf\n",
    "    output:plain text\n",
    "    main packages: os\n",
    "    user_story=get_text(full_file)\n",
    "    '''\n",
    "    file1=open(full_file,'r',encoding='windows-1252')\n",
    "    user_story_lines = file1.readlines()\n",
    "    user_story2=\"\"\n",
    "    for line in user_story_lines:\n",
    "        user_story2=user_story2+str(line)\n",
    "    user_story22=\" \".join(user_story2.split('\\n'))\n",
    "    return user_story22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(user_story):\n",
    "  clean_user_story=clean(user_story,\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"\",\n",
    "    replace_with_email=\"\",\n",
    "    replace_with_phone_number=\"\",\n",
    "    replace_with_number=\"\",\n",
    "    replace_with_digit=\"\",\n",
    "    replace_with_currency_symbol=\"\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "\n",
    "  return clean_user_story"
   ]
  },
  {
   "source": [
    "## Exploratory Data Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text(clean_user_story):\n",
    "    rnlp=spacy.load('en_core_web_sm')\n",
    "    rnlp.add_pipe(rme_custom_sents,before='parser')\n",
    "    #rme_df=pd.DataFrame(columns=['id','sentence','role','mean','end','start_idx','end_idx','lable','token_text'])\n",
    "    nlp2=spacy.load('en_core_web_sm')\n",
    "    #nlp2.add_pipe(custom_sents,before='parser')\n",
    "    docc=nlp2(clean_user_story)\n",
    "    rme_df=pd.DataFrame()\n",
    "    i=0\n",
    "    id_ls=[]\n",
    "    sen_ls=[]\n",
    "    for sent_i,sc in enumerate(docc.sents):\n",
    "        id_ls.append(sent_i+1)\n",
    "        sen_ls.append(sc.text.strip('|').strip())\n",
    "    rme_df['id']=id_ls\n",
    "    rme_df['sentence']=sen_ls\n",
    "\n",
    "\n",
    "\n",
    "    ent_df=pd.DataFrame()\n",
    "    #entities_dic = {} \n",
    "    C2_start=[]\n",
    "    C2_end=[]\n",
    "    C2_lable=[]\n",
    "    C2_text=[]\n",
    "    C2_sent=[]\n",
    "    c2_matcher=Matcher(nlp2.vocab)\n",
    "    c2_pattern = [{'POS': 'NOUN'}]\n",
    "    docc=nlp2(clean_user_story)\n",
    "    c2_matcher.add('c2_pattern',None,c2_pattern)\n",
    "\n",
    "    C3_start=[]\n",
    "    C3_end=[]\n",
    "    C3_lable=[]\n",
    "    C3_text=[]\n",
    "    C3_sent=[]\n",
    "    c3_matcher=Matcher(nlp2.vocab)\n",
    "    c3_pattern = [{'DEP': 'nsubj','POS': 'NOUN'}]\n",
    "    c3_matcher.add('c3_pattern',None,c3_pattern)\n",
    "\n",
    "    C4_start=[]\n",
    "    C4_end=[]\n",
    "    C4_lable=[]\n",
    "    C4_text=[]\n",
    "    C4_sent=[]\n",
    "    c4_matcher=Matcher(nlp2.vocab)\n",
    "    c4_pattern = [{'DEP': 'compound'}]\n",
    "    c4_matcher.add('c4_pattern',None,c4_pattern)\n",
    "\n",
    "    H2_start=[]\n",
    "    H2_end=[]\n",
    "    H2_lable=[]\n",
    "    H2_text=[]\n",
    "    H2_sent=[]\n",
    "\n",
    "    R5_start=[]\n",
    "    R5_end=[]\n",
    "    R5_lable=[]\n",
    "    R5_text=[]\n",
    "    R5_sent=[]\n",
    "    #for match_id,start,end in c2_matcher(docc):\n",
    "        #span=docc[start:end]#C2\n",
    "        #C2_start.append(span.start_char)\n",
    "        #C2_end.append(span.end_char)\n",
    "        #C2_lable.append(\"C2\")\n",
    "        #C2_text.append(span.text)\n",
    "        #C2_sent.append(span.sent.text)\n",
    "\n",
    "    for match_id,start,end in c3_matcher(docc):\n",
    "        span=docc[start:end]#C3\n",
    "        C3_start.append(span.start_char)\n",
    "        C3_end.append(span.end_char)\n",
    "        C3_lable.append(\"C3\")\n",
    "        C3_text.append(span.text)\n",
    "        C3_sent.append(span.sent.text)\n",
    "\n",
    "    ent_df['start_idx']=C3_start+C4_start\n",
    "    ent_df['end_idx']=C3_end+C4_end\n",
    "    ent_df['lable']=C3_lable+C4_lable\n",
    "    #ent_df['text']=C2_text+C3_text+C4_text+H2_text+R5_text\n",
    "    #ent_df['sentence']=C2_sent+C3_sent+C4_sent+H2_sent+R5_sent\n",
    "    ent_df\n",
    "    #temp_df=pd.merge(rme_df, ent_df, on='sentence')\n",
    "    #temp_df\n",
    "\n",
    "    rel_df=pd.DataFrame()\n",
    "    V_start=[]\n",
    "    V_end=[]\n",
    "    V_lable=[]\n",
    "    V_text=[]\n",
    "    V_sent=[]\n",
    "    V_matcher=Matcher(nlp2.vocab)\n",
    "    v_pattern = [{'POS': 'VERB'}]\n",
    "    #docc=nlp2(clean_user_story)\n",
    "    V_matcher.add('v_pattern',None,v_pattern)\n",
    "\n",
    "    Vp_start=[]\n",
    "    Vp_end=[]\n",
    "    Vp_lable=[]\n",
    "    Vp_text=[]\n",
    "    Vp_sent=[]\n",
    "    Vp_matcher=Matcher(nlp2.vocab)\n",
    "    vp_pattern = [{'POS': 'VERB'},\n",
    "                    {'DEP':'prep'}]\n",
    "    #docc=nlp2(clean_user_story)\n",
    "    Vp_matcher.add('vp_pattern',None,vp_pattern)\n",
    "\n",
    "    dobj_start=[]\n",
    "    dobj_end=[]\n",
    "    dobj_lable=[]\n",
    "    dobj_text=[]\n",
    "    dobj_sent=[]\n",
    "    dobj_matcher=Matcher(nlp2.vocab)\n",
    "    dobj_pattern = [{'DEP': 'dobj', 'POS': 'NOUN'}]\n",
    "    dobj_matcher.add('dobj_pattern',None,dobj_pattern)\n",
    "\n",
    "    iobj_start=[]\n",
    "    iobj_end=[]\n",
    "    iobj_lable=[]\n",
    "    iobj_text=[]\n",
    "    iobj_sent=[]\n",
    "    iobj_matcher=Matcher(nlp2.vocab)\n",
    "    iobj_pattern = [{'DEP': 'iobj', 'POS': 'NOUN'}]\n",
    "    iobj_matcher.add('iobj_pattern',None,iobj_pattern)\n",
    "\n",
    "    pobj_start=[]\n",
    "    pobj_end=[]\n",
    "    pobj_lable=[]\n",
    "    pobj_text=[]\n",
    "    pobj_sent=[]\n",
    "    pobj_matcher=Matcher(nlp2.vocab)\n",
    "    pobj_pattern = [{'DEP': 'pobj', 'POS': 'NOUN'}]\n",
    "    pobj_matcher.add('pobj_pattern',None,pobj_pattern)\n",
    "\n",
    "    for match_id,start,end in V_matcher(docc):\n",
    "        span=docc[start:end]\n",
    "        V_start.append(span.start_char)\n",
    "        V_end.append(span.end_char)\n",
    "        V_lable.append(\"R2\")\n",
    "        V_text.append(span.text)\n",
    "        V_sent.append(span.sent.text)\n",
    "        #for token in span:\n",
    "            #if check_verb(token)=='TRANVERB':\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    #for match_id,start,end in Vp_matcher(docc):\n",
    "        #   span=docc[start:end]#R4\n",
    "        #  Vp_start.append(span.start_char)\n",
    "        # Vp_end.append(span.end_char)\n",
    "        # Vp_lable.append(\"R4\")\n",
    "        # Vp_text.append(span.text)\n",
    "        #Vp_sent.append(span.sent.text)\n",
    "\n",
    "\n",
    "    for match_id,start,end in dobj_matcher(docc):\n",
    "        span=docc[start:end]\n",
    "        dobj_start.append(span.start_char)\n",
    "        dobj_end.append(span.end_char)\n",
    "        dobj_lable.append(\"C2\")\n",
    "        dobj_text.append(span.text)\n",
    "        dobj_sent.append(span.sent.text)\n",
    "\n",
    "    for match_id,start,end in pobj_matcher(docc):\n",
    "        span=docc[start:end]\n",
    "        pobj_start.append(span.start_char)\n",
    "        pobj_end.append(span.end_char)\n",
    "        pobj_lable.append(\"C2\")\n",
    "        pobj_text.append(span.text)\n",
    "        pobj_sent.append(span.sent.text)    \n",
    "\n",
    "    rel_df['start_idx']=V_start+dobj_start+pobj_start\n",
    "    rel_df['end_idx']=V_end+dobj_end+pobj_start\n",
    "    rel_df['lable']=V_lable+dobj_lable+pobj_lable\n",
    "    #rel_df['text']=V_text+Vp_text+dobj_text+pobj_text\n",
    "    #rel_df['sentence']=V_sent+Vp_sent+dobj_sent+pobj_sent\n",
    "    #rel_temp_df=pd.merge(rme_df, rel_df, on='sentence')\n",
    "    #full_df=pd.concat([temp_df,rel_temp_df])\n",
    "    #full_df=full_df.drop_duplicates()\n",
    "\n",
    "    full_df1=pd.concat([ent_df,rel_df])\n",
    "    full_df1=full_df1.drop_duplicates()\n",
    "    label_ls=[]\n",
    "    for index,row in full_df1.iterrows():\n",
    "        label_ls.append((row['start_idx'],row['end_idx'],row['lable']))\n",
    "    label_dic={}\n",
    "    label_dic['entities']=label_ls\n",
    "\n",
    "\n",
    "    return label_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(train_data_tp):\n",
    "    #train_data_tp=(clean_user_story,labels_dic)\n",
    "    text_file = open(\"train_data.txt\", \"w\")\n",
    "    text_file.write(str(train_data_tp))\n",
    "    text_file.close()\n",
    "    return text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='train_data.txt' mode='w' encoding='UTF-8'>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "rootdir='/Users/alqahtsa/py_projects/uml_generator/user_stories_txt'\n",
    "#splitting data\n",
    "train_set=math.floor(len([file for file in os.listdir(rootdir)])*.7)# 70% of tha data \n",
    "\n",
    "train_data_ls=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for i,file in enumerate(files):\n",
    "        if i<train_set:\n",
    "            filepath = subdir + os.sep + file\n",
    "            user_story=get_text(filepath)\n",
    "            clean_user_story=preprocess_text(user_story)\n",
    "            labels_dic=annotate_text(clean_user_story)\n",
    "            train_data_tp=(clean_user_story,labels_dic)\n",
    "            train_data_ls.append((train_data_tp))\n",
    "generate_train_data(train_data_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file=open('train_data.pkl','wb')\n",
    "pickle.dump(train_data_ls,pkl_file)\n",
    "pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pickle.load(open('train_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp10=spacy.blank('en')\n",
    "def train_model(train_data):\n",
    "    if 'ner' not in nlp10.pipe_names:\n",
    "        ner=nlp10.create_pipe('ner')\n",
    "        nlp10.add_pipe(ner,last=True)\n",
    "    for _,annotation in train_data:\n",
    "        for ent in annotation['entities']:\n",
    "            ner.add_label(ent[2])\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp10.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp10.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        \n",
    "        nlp10.begin_training()\n",
    "        for itn in range(30):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp10.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/alqahtsa/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:635: UserWarning: [W033] Training a new parser or NER using a model with an empty lexeme normalization table. This may degrade the performance to some degree. If this is intentional or this language doesn't have a normalization table, please ignore this warning.\n",
      "  proc.begin_training(\n",
      "/Users/alqahtsa/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:635: UserWarning: [W034] Please install the package spacy-lookups-data in order to include the default lexeme normalization table for the language 'en'.\n",
      "  proc.begin_training(\n",
      "Losses {'ner': 26684.57846069336}\n",
      "Losses {'ner': 19113.793411254883}\n",
      "Losses {'ner': 11219.069913864136}\n",
      "Losses {'ner': 10911.942171096802}\n",
      "Losses {'ner': 10916.95958328247}\n",
      "Losses {'ner': 10110.30926322937}\n",
      "Losses {'ner': 9498.654777526855}\n",
      "Losses {'ner': 8477.914203643799}\n",
      "Losses {'ner': 8076.770166397095}\n",
      "Losses {'ner': 7197.0363845825195}\n",
      "Losses {'ner': 7090.1977071762085}\n",
      "Losses {'ner': 6427.361822605133}\n",
      "Losses {'ner': 5912.616696357727}\n",
      "Losses {'ner': 5623.343173742294}\n",
      "Losses {'ner': 5477.0849142074585}\n",
      "Losses {'ner': 4835.527111530304}\n",
      "Losses {'ner': 4811.207501173019}\n",
      "Losses {'ner': 4377.280557155609}\n",
      "Losses {'ner': 3970.2379789352417}\n",
      "Losses {'ner': 3767.6652598381042}\n",
      "Losses {'ner': 3509.750184059143}\n",
      "Losses {'ner': 3200.132840871811}\n",
      "Losses {'ner': 3229.917339324951}\n",
      "Losses {'ner': 2894.1914558410645}\n",
      "Losses {'ner': 2834.4989132881165}\n",
      "Losses {'ner': 2636.183746576309}\n",
      "Losses {'ner': 2667.0631017684937}\n",
      "Losses {'ner': 2457.1908423900604}\n",
      "Losses {'ner': 2434.8717761039734}\n",
      "Losses {'ner': 2300.6159224510193}\n"
     ]
    }
   ],
   "source": [
    "train_model(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model=spacy.load('ner_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2                            - want\nR2                            - deposit\nR2                            - maintain\nC2                            - datasets\nR2                            - need\nR2                            - install\nR2                            - learn\nC2                            - software\nR2                            - deposit\nR2                            - want\nC2                            - interface\nR2                            - feel\nR2                            - like\nR2                            - joined\nR2                            - want\nR2                            - deposit\nR2                            - maintain\nC2                            - datasets\nR2                            - managing\nC2                            - outputs\nR2                            - want\nR2                            - deposit\nR2                            - maintain\nC2                            - datasets\nR2                            - continue\nR2                            - work\nR2                            - want\nR2                            - deposit\nC2                            - files\nR2                            - spend\nC2                            - lot\nR2                            - finding\nC2                            - version\nR2                            - converting\nR2                            - want\nR2                            - place\nC2                            - embargo\nR2                            - use\nR2                            - protected\nR2                            - fulfil\nC2                            - responsibilities\nR2                            - want\nR2                            - apply\nC2                            - licenses\nR2                            - protected\nR2                            - want\nR2                            - allow\nR2                            - privileged\nC2                            - access\nR2                            - continue\nC2                            - relationship\nR2                            - want\nR2                            - deposit\nC2                            - files\nR2                            - limited\nR2                            - deposit\nR2                            - want\nR2                            - link\nC2                            - datasets\nR2                            - discovered\nR2                            - want\nR2                            - mint\nR2                            - discovered\nR2                            - cited\nR2                            - tracked\nR2                            - receive\nC2                            - credit\nR2                            - want\nC2                            - metadata\nR2                            - filled\nR2                            - remembered\nR2                            - waste\nR2                            - reentering\nC2                            - information\nR2                            - want\nR2                            - link\nR2                            - data\nR2                            - stored\nR2                            - store\nC2                            - data\nR2                            - register\nR2                            - deposit\nC2                            - data\nR2                            - want\nR2                            - specify\nC2                            - policy\nR2                            - breach\nC2                            - laws\nR2                            - want\nR2                            - track\nC2                            - downloads\nR2                            - demonstrate\nC2                            - impact\nR2                            - want\nR2                            - track\nC2                            - citations\nR2                            - demonstrate\nC2                            - impact\nR2                            - want\nR2                            - guarantees\nR2                            - use\nC2                            - data\nR2                            - fulfil\nC2                            - requirements\nR2                            - want\nR2                            - attach\nC2                            - metadata\nR2                            - find\nC2                            - data\nR2                            - want\nR2                            - link\nC2                            - datasets\nR2                            - demonstrated\nR2                            - linked\nR2                            - want\nR2                            - manage\nR2                            - share\nC2                            - live\nR2                            - linked\nR2                            - want\nR2                            - manage\nC2                            - versions\nR2                            - changes\nR2                            - compromise\nC2                            - integrity\nR2                            - want\nR2                            - allow\nC2                            - others\nR2                            - deposit\nR2                            - delegate\nC2                            - tasks\nR2                            - want\nR2                            - search\nC2                            - archive\nR2                            - find\nC2                            - relevant\nR2                            - want\nR2                            - access\nC2                            - system\nR2                            - put\nR2                            - reusing\nC2                            - university\nR2                            - want\nR2                            - examine\nR2                            - identify\nC2                            - files\nR2                            - make\nC2                            - assessment\nR2                            - downloading\nC2                            - dataset\nR2                            - want\nR2                            - view\nC2                            - citation\nR2                            - reference\nR2                            - want\nR2                            - view\nC2                            - doi\nR2                            - import\nC2                            - dataset\nR2                            - want\nC2                            - url\nR2                            - want\nR2                            - search\nC2                            - archive\nR2                            - search\nC2                            - books\nR2                            - want\nR2                            - see\nC2                            - versions\nR2                            - using\nC2                            - version\nR2                            - want\nR2                            - gain\nC2                            - access\nR2                            - data\nR2                            - involved\nR2                            - collaborate\nR2                            - want\nR2                            - guarantees\nR2                            - breached\nC2                            - risk\nR2                            - collaborating\nR2                            - want\nR2                            - access\nC2                            - data\nR2                            - collaborate\nR2                            - want\nR2                            - deposit\nC2                            - data\nR2                            - required\nR2                            - maintain\nC2                            - archive\nR2                            - access\nC2                            - data\nR2                            - needed\nR2                            - want\nR2                            - make\nC2                            - checks\nR2                            - deposited\nC2                            - datasets\nR2                            - made\nC2                            - quality\nR2                            - maintained\nR2                            - checked\nR2                            - details\nR2                            - licensing\nR2                            - checked\nR2                            - want\nR2                            - require\nC2                            - set\nR2                            - maintained\nR2                            - want\nR2                            - approve\nC2                            - disposal\nR2                            - required\nR2                            - destroyed\nR2                            - want\nR2                            - query\nC2                            - archive\nR2                            - report\nR2                            - want\nR2                            - import\nC2                            - data\nR2                            - lost\nR2                            - close\nC2                            - down\nR2                            - want\nR2                            - encourage\nR2                            - promote\nC2                            - use\nR2                            - want\nR2                            - integrate\nC2                            - archive\nR2                            - analyse\nC2                            - impact\nR2                            - link\nC2                            - funding\nR2                            - produces\nR2                            - want\nR2                            - include\nC2                            - records\nR2                            - held\nC2                            - complete\nR2                            - want\nR2                            - track\nC2                            - counts\nR2                            - published\nC2                            - datasets\nR2                            - demonstrated\nR2                            - want\nR2                            - segment\nC2                            - view\nR2                            - download\nC2                            - statistics\nR2                            - demonstrated\nR2                            - want\nR2                            - linked\nR2                            - report\nR2                            - depositing\nC2                            - datasets\nR2                            - funder\nC2                            - requirements\nR2                            - want\nR2                            - store\nC2                            - data\nR2                            - existing\nC2                            - systems\nR2                            - guaranteed\nR2                            - want\nR2                            - integrate\nC2                            - archive\nR2                            - existing\nC2                            - systems\nR2                            - administering\nC2                            - system\nR2                            - kept\nC2                            - low\nR2                            - want\nR2                            - store\nC2                            - data\nR2                            - made\nR2                            - want\nR2                            - export\nC2                            - data\nR2                            - tied\nR2                            - want\nR2                            - deposit\nR2                            - maintain\nC2                            - datasets\nR2                            - interact\nR2                            - want\nR2                            - make\nC2                            - links\nR2                            - underlying\nC2                            - datasets\nR2                            - seen\nR2                            - filled\nR2                            - want\nR2                            - reassured\nC2                            - researchers\nR2                            - fund\nC2                            - plans\nR2                            - funding\nR2                            - want\nR2                            - harvest\nC2                            - metadata\nR2                            - analyse\nC2                            - effectiveness\nR2                            - encourage\nC2                            - fertilisation\n"
     ]
    }
   ],
   "source": [
    "p='/Users/alqahtsa/py_projects/uml_generator/user_stories_txt/g05-openspending.txt'\n",
    "test_user_story=get_text(p)\n",
    "test_clean_user_story=preprocess_text(user_story)\n",
    "tdoc=ner_model(test_clean_user_story)\n",
    "for ent in tdoc.ents:\n",
    "    print(f'{ent.label_.upper():{30}}- {ent.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sents(docx):\n",
    "    for token in docx[:-1]:\n",
    "        if token.text=='|':\n",
    "            docx[token.i].is_sent_start=True\n",
    "    return docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rme_custom_sents(docx):\n",
    "    for token in docx[:-1]:\n",
    "        if token.text==',' and docx[token.i+1].text=='i':\n",
    "            docx[token.i].is_sent_start=True\n",
    "        elif token.text==',' and docx[token.i+1].text=='so':\n",
    "            docx[token.i].is_sent_start=True\n",
    "    return docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_verb(token):\n",
    "    \"\"\"Check verb type given spacy token\"\"\"\n",
    "    if token.pos_ == 'VERB':\n",
    "        indirect_object = False\n",
    "        direct_object = False\n",
    "        for item in token.children:\n",
    "            if(item.dep_ == \"iobj\" or item.dep_ == \"pobj\"):\n",
    "                indirect_object = True\n",
    "            if (item.dep_ == \"dobj\" or item.dep_ == \"dative\"):\n",
    "                direct_object = True\n",
    "        if indirect_object and direct_object:\n",
    "            return 'DITRANVERB'\n",
    "        elif direct_object and not indirect_object:\n",
    "            return 'TRANVERB'\n",
    "        elif not direct_object and not indirect_object:\n",
    "            return 'INTRANVERB'\n",
    "        else:\n",
    "            return 'VERB'\n",
    "    else:\n",
    "        return token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}